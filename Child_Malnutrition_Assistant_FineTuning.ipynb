{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf64f56",
   "metadata": {},
   "source": [
    "# Child Malnutrition Assistant - Fine-Tuning with LoRA\n",
    "\n",
    "Domain: child malnutrition advice, support, and balanced diet guidance.\n",
    "This notebook is designed to run end-to-end on Google Colab.\n",
    "Detailed narrative documentation is provided in the PDF report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca8bf61",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Run the installation cell once if needed in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319226a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab, uncomment the next line.\n",
    "# !pip install -q transformers datasets peft trl bitsandbytes accelerate evaluate rouge_score sentencepiece streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from evaluate import load\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"malnutrition_dataset_final.jsonl\"\n",
    "GITHUB_RAW_URL = (\n",
    "    \"https://raw.githubusercontent.com/pauline12ish34/\"\n",
    "    \"summative_fine-tuning_LLM/main/malnutrition_dataset_final.jsonl\"\n",
    ")\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "SEED = 42\n",
    "MAX_SEQ_LENGTH = 512\n",
    "EVAL_SAMPLES = 20\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f81abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from GitHub if not present\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    import urllib.request\n",
    "\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(GITHUB_RAW_URL, DATA_PATH)\n",
    "    print(f\"Saved to {DATA_PATH}\")\n",
    "else:\n",
    "    print(f\"Dataset already available at {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def format_example(question: str, answer: str) -> Dict[str, str]:\n",
    "    return {\n",
    "        \"instruction\": question,\n",
    "        \"response\": answer,\n",
    "        \"text\": f\"### Question: {question}\\n\\n### Answer: {answer}\",\n",
    "    }\n",
    "\n",
    "def load_jsonl_dataset(file_path: str) -> List[Dict[str, str]]:\n",
    "    data: List[Dict[str, str]] = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "                if \"messages\" in item:\n",
    "                    user_msg = item[\"messages\"][0][\"content\"]\n",
    "                    assistant_msg = item[\"messages\"][1][\"content\"]\n",
    "                elif \"question\" in item and \"answer\" in item:\n",
    "                    user_msg = item[\"question\"]\n",
    "                    assistant_msg = item[\"answer\"]\n",
    "                else:\n",
    "                    user_msg = item.get(\"instruction\")\n",
    "                    assistant_msg = item.get(\"response\")\n",
    "                if not user_msg or not assistant_msg:\n",
    "                    continue\n",
    "                user_msg = normalize_text(user_msg)\n",
    "                assistant_msg = normalize_text(assistant_msg)\n",
    "                data.append(format_example(user_msg, assistant_msg))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Skipping line {line_num} due to JSON error\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0fedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_jsonl_dataset(DATA_PATH)\n",
    "if not raw_data:\n",
    "    raise ValueError(\"Dataset is empty or could not be loaded.\")\n",
    "\n",
    "df = pd.DataFrame(raw_data)\n",
    "df = df.dropna(subset=[\"instruction\", \"response\", \"text\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Samples loaded: {len(df)}\")\n",
    "print(df.head(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f9ea4",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "Tokenization uses the model's native tokenizer (SentencePiece/BPE for TinyLlama, not WordPiece).\n",
    "Text is normalized using Unicode NFKC and whitespace cleanup.\n",
    "Full preprocessing documentation is provided in the PDF report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9df006",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"instruction\", \"response\", \"text\"]])\n",
    "split = dataset.train_test_split(test_size=0.15, seed=SEED)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Eval size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61825436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token length analysis for sequence length selection\n",
    "token_lengths = []\n",
    "for example in train_dataset.select(range(min(200, len(train_dataset)))):\n",
    "    token_lengths.append(len(tokenizer.encode(example[\"text\"])))\n",
    "\n",
    "token_lengths = np.array(token_lengths)\n",
    "print(f\"Mean length: {token_lengths.mean():.1f}\")\n",
    "print(f\"95th percentile: {np.percentile(token_lengths, 95):.0f}\")\n",
    "print(f\"Max length: {token_lengths.max()}\")\n",
    "print(f\"Max seq length used: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load base model for baseline evaluation and fine-tuning\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "print(\"Base model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(model, questions: List[str], max_new_tokens: int = 150) -> List[str]:\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    outputs = []\n",
    "    for q in questions:\n",
    "        prompt = f\"### Question: {q}\\n\\n### Answer:\"\n",
    "        result = gen_pipe(prompt)[0][\"generated_text\"]\n",
    "        answer = result.split(\"### Answer:\")[1].strip() if \"### Answer:\" in result else result\n",
    "        outputs.append(answer)\n",
    "    return outputs\n",
    "\n",
    "def compute_f1(preds: List[str], refs: List[str]) -> float:\n",
    "    scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = pred.lower().split()\n",
    "        ref_tokens = ref.lower().split()\n",
    "        common = set(pred_tokens) & set(ref_tokens)\n",
    "        if not pred_tokens or not ref_tokens:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        precision = len(common) / len(pred_tokens) if pred_tokens else 0.0\n",
    "        recall = len(common) / len(ref_tokens) if ref_tokens else 0.0\n",
    "        if precision + recall == 0:\n",
    "            scores.append(0.0)\n",
    "        else:\n",
    "            scores.append(2 * precision * recall / (precision + recall))\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "bleu_metric = load(\"bleu\")\n",
    "rouge_metric = load(\"rouge\")\n",
    "\n",
    "eval_subset = eval_dataset.select(range(min(EVAL_SAMPLES, len(eval_dataset))))\n",
    "baseline_questions = eval_subset[\"instruction\"]\n",
    "baseline_refs = eval_subset[\"response\"]\n",
    "\n",
    "baseline_preds = generate_answers(base_model, baseline_questions)\n",
    "baseline_bleu = bleu_metric.compute(\n",
    "    predictions=baseline_preds,\n",
    "    references=[[r] for r in baseline_refs],\n",
    ")\n",
    "baseline_rouge = rouge_metric.compute(\n",
    "    predictions=baseline_preds,\n",
    "    references=baseline_refs,\n",
    ")\n",
    "baseline_f1 = compute_f1(baseline_preds, baseline_refs)\n",
    "\n",
    "BASELINE_METRICS = {\n",
    "    \"bleu\": baseline_bleu[\"bleu\"],\n",
    "    \"rouge1\": baseline_rouge[\"rouge1\"],\n",
    "    \"rouge2\": baseline_rouge[\"rouge2\"],\n",
    "    \"rougeL\": baseline_rouge[\"rougeL\"],\n",
    "    \"f1\": baseline_f1,\n",
    "}\n",
    "\n",
    "print(\"Baseline metrics:\", BASELINE_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30dd4c",
   "metadata": {},
   "source": [
    "## LoRA Fine-Tuning and Hyperparameter Experiments\n",
    "Run multiple configurations and compare results. This cell produces the experiment table and improvement evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ffab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"baseline\",\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 4,\n",
    "        \"gradient_accumulation\": 4,\n",
    "        \"epochs\": 2,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"low_lr\",\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"batch_size\": 4,\n",
    "        \"gradient_accumulation\": 4,\n",
    "        \"epochs\": 2,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"batch2\",\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation\": 8,\n",
    "        \"epochs\": 2,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"higher_rank\",\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 4,\n",
    "        \"gradient_accumulation\": 4,\n",
    "        \"epochs\": 2,\n",
    "        \"lora_r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001fa15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment runner\n",
    "experiment_results = []\n",
    "\n",
    "\n",
    "def pct_improvement(base: float, new: float) -> float:\n",
    "    if base == 0:\n",
    "        return 0.0\n",
    "    return (new - base) / base * 100\n",
    "\n",
    "\n",
    "def run_experiment(exp: Dict[str, float]) -> Dict[str, float]:\n",
    "    lora_config = LoraConfig(\n",
    "        r=exp[\"lora_r\"],\n",
    "        lora_alpha=exp[\"lora_alpha\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{exp['name']}\",\n",
    "        num_train_epochs=exp[\"epochs\"],\n",
    "        per_device_train_batch_size=exp[\"batch_size\"],\n",
    "        per_device_eval_batch_size=exp[\"batch_size\"],\n",
    "        gradient_accumulation_steps=exp[\"gradient_accumulation\"],\n",
    "        learning_rate=exp[\"learning_rate\"],\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps=10,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        peft_config=lora_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    training_minutes = (time.time() - start_time) / 60\n",
    "    max_gpu_gb = None\n",
    "    if torch.cuda.is_available():\n",
    "        max_gpu_gb = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "\n",
    "    fine_preds = generate_answers(model, baseline_questions)\n",
    "    fine_bleu = bleu_metric.compute(\n",
    "        predictions=fine_preds,\n",
    "        references=[[r] for r in baseline_refs],\n",
    "    )\n",
    "    fine_rouge = rouge_metric.compute(\n",
    "        predictions=fine_preds,\n",
    "        references=baseline_refs,\n",
    "    )\n",
    "    fine_f1 = compute_f1(fine_preds, baseline_refs)\n",
    "    eval_results = trainer.evaluate()\n",
    "    perplexity = math.exp(eval_results[\"eval_loss\"])\n",
    "\n",
    "    best_checkpoint = trainer.state.best_model_checkpoint\n",
    "    if best_checkpoint:\n",
    "        best_dir = best_checkpoint\n",
    "    else:\n",
    "        best_dir = f\"./best_{exp['name']}\"\n",
    "        trainer.save_model(best_dir)\n",
    "\n",
    "    tokenizer.save_pretrained(best_dir)\n",
    "\n",
    "    return {\n",
    "        \"Experiment\": exp[\"name\"],\n",
    "        \"Learning Rate\": exp[\"learning_rate\"],\n",
    "        \"Batch Size\": exp[\"batch_size\"],\n",
    "        \"Grad Accum\": exp[\"gradient_accumulation\"],\n",
    "        \"Epochs\": exp[\"epochs\"],\n",
    "        \"BLEU\": fine_bleu[\"bleu\"],\n",
    "        \"ROUGE-L\": fine_rouge[\"rougeL\"],\n",
    "        \"F1\": fine_f1,\n",
    "        \"Perplexity\": perplexity,\n",
    "        \"BLEU Improvement %\": pct_improvement(BASELINE_METRICS[\"bleu\"], fine_bleu[\"bleu\"]),\n",
    "        \"ROUGE-L Improvement %\": pct_improvement(BASELINE_METRICS[\"rougeL\"], fine_rouge[\"rougeL\"]),\n",
    "        \"Training Time (min)\": training_minutes,\n",
    "        \"Max GPU (GB)\": max_gpu_gb,\n",
    "        \"Best Checkpoint\": best_dir,\n",
    "    }\n",
    "\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"Running: {exp['name']}\")\n",
    "    result = run_experiment(exp)\n",
    "    experiment_results.append(result)\n",
    "\n",
    "exp_df = pd.DataFrame(experiment_results)\n",
    "print(exp_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2da9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evidence of improvement\n",
    "if len(experiment_results) > 0:\n",
    "    best_improvement = exp_df[\"ROUGE-L Improvement %\"].max()\n",
    "    print(f\"Best ROUGE-L improvement: {best_improvement:.2f}%\")\n",
    "    if best_improvement >= 10:\n",
    "        print(\"Improvement target met (>= 10%).\")\n",
    "    else:\n",
    "        print(\"Improvement target not met yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative test using the last trained model\n",
    "sample_question = \"What are early signs of child malnutrition?\"\n",
    "sample_answer = generate_answers(model, [sample_question], max_new_tokens=120)[0]\n",
    "print(\"Question:\", sample_question)\n",
    "print(\"Answer:\", sample_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0187eb0",
   "metadata": {},
   "source": [
    "## UI Integration (Streamlit)\n",
    "The Streamlit app is available in app.py for interactive testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab395a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_jsonl_dataset(DATA_PATH)\n",
    "if not raw_data:\n",
    "    raise ValueError(\"Dataset is empty or could not be loaded.\")\n",
    "\n",
    "df = pd.DataFrame(raw_data)\n",
    "df = df.dropna(subset=[\"instruction\", \"response\", \"text\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Samples loaded: {len(df)}\")\n",
    "print(df.head(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7dd0bc",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "Tokenization uses the model's native tokenizer (SentencePiece/BPE for TinyLlama, not WordPiece).\n",
    "Text is normalized using Unicode NFKC and whitespace cleanup.\n",
    "Full preprocessing documentation is provided in the PDF report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"instruction\", \"response\", \"text\"]])\n",
    "split = dataset.train_test_split(test_size=0.15, seed=SEED)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Eval size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa4118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token length analysis for sequence length selection\n",
    "token_lengths = []\n",
    "for example in train_dataset.select(range(min(200, len(train_dataset)))):\n",
    "    token_lengths.append(len(tokenizer.encode(example[\"text\"])))\n",
    "\n",
    "token_lengths = np.array(token_lengths)\n",
    "print(f\"Mean length: {token_lengths.mean():.1f}\")\n",
    "print(f\"95th percentile: {np.percentile(token_lengths, 95):.0f}\")\n",
    "print(f\"Max length: {token_lengths.max()}\")\n",
    "print(f\"Max seq length used: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36404df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load base model for baseline evaluation and fine-tuning\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "print(\"Base model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c8703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(model, questions: List[str], max_new_tokens: int = 150) -> List[str]:\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "    'f1': fine_f1,\n",
    "    'perplexity': perplexity,\n",
    "}\n",
    "\n",
    "print('Fine-tuned metrics:', FINETUNED_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e62d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment tracking table\n",
    "experiment_results = []\n",
    "\n",
    "def pct_improvement(base: float, new: float) -> float:\n",
    "    if base == 0:\n",
    "        return 0.0\n",
    "    return (new - base) / base * 100\n",
    "\n",
    "experiment_results.append({\n",
    "    'Experiment': current_exp['name'],\n",
    "    'Learning Rate': current_exp['learning_rate'],\n",
    "    'Batch Size': current_exp['batch_size'],\n",
    "    'Grad Accum': current_exp['gradient_accumulation'],\n",
    "    'Epochs': current_exp['epochs'],\n",
    "    'BLEU': FINETUNED_METRICS['bleu'],\n",
    "    'ROUGE-L': FINETUNED_METRICS['rougeL'],\n",
    "    'F1': FINETUNED_METRICS['f1'],\n",
    "    'Perplexity': FINETUNED_METRICS['perplexity'],\n",
    "    'BLEU Improvement %': pct_improvement(BASELINE_METRICS['bleu'], FINETUNED_METRICS['bleu']),\n",
    "    'ROUGE-L Improvement %': pct_improvement(BASELINE_METRICS['rougeL'], FINETUNED_METRICS['rougeL']),\n",
    "    'Training Time (min)': training_minutes,\n",
    "})\n",
    "\n",
    "exp_df = pd.DataFrame(experiment_results)\n",
    "print(exp_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative test\n",
    "sample_question = 'What are early signs of child malnutrition?'\n",
    "sample_answer = generate_answers(model, [sample_question], max_new_tokens=120)[0]\n",
    "print('Question:', sample_question)\n",
    "print('Answer:', sample_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "output_dir = f'./malnutrition_lora_{current_exp[\n",
    "]}'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print('Saved to:', output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eab05a",
   "metadata": {},
   "source": [
    "# Child Malnutrition Assistant - Fine-Tuning with LoRA\n",
    "\n",
    "Fine-tuning TinyLlama-1.1B on child malnutrition medical Q&A dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369e244",
   "metadata": {},
   "source": [
    "# Child Malnutrition Assistant - Fine-tuning with LoRA\n",
    "\n",
    "This notebook demonstrates fine-tuning of TinyLlama-1.1B-Chat-v1.0 using LoRA (Low-Rank Adaptation) on child malnutrition data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e1ab3",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51adf99e",
   "metadata": {},
   "source": [
    "## Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "print(f'CUDA Available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f046d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08559cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_jsonl(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset_from_jsonl('malnutrition_dataset_final.jsonl')\n",
    "    print(f'Loaded {len(dataset)} samples')\n",
    "    if dataset:\n",
    "        print(f'Sample: {dataset[0]}')\n",
    "except FileNotFoundError:\n",
    "    print('Dataset file not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39609c",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b7ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print('BitsAndBytes config created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a1167",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f'Model loaded: {MODEL_NAME}')\n",
    "print(f'Model parameters: {model.num_parameters():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a88d30",
   "metadata": {},
   "source": [
    "## LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e557b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print('LoRA applied to model')\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65561b",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(sample):\n",
    "    return {\n",
    "        'text': f\"<s>[INST] {sample.get('question', '')} [/INST] {sample.get('answer', '')} </s>\"\n",
    "    }\n",
    "\n",
    "if dataset:\n",
    "    formatted_data = [format_chat_template(sample) for sample in dataset]\n",
    "    train_dataset = Dataset.from_dict({'text': [d['text'] for d in formatted_data]})\n",
    "    print(f'Training dataset prepared with {len(train_dataset)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7bc23b",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'output_dir': './output',\n",
    "    'num_train_epochs': 3,\n",
    "    'per_device_train_batch_size': 4,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'learning_rate': 2e-4,\n",
    "    'logging_steps': 10,\n",
    "    'save_steps': 100,\n",
    "}\n",
    "\n",
    "print('Training configuration ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc3355",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890dec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset:\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        args=training_args,\n",
    "        peft_config=lora_config,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    print('Trainer initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ad2fd",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa541f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset:\n",
    "    trainer.train()\n",
    "    print('Training completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4555b74",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbb5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(question):\n",
    "    prompt = f\"[INST] {question} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(DEVICE)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    return response\n",
    "\n",
    "test_question = \"What are symptoms of child malnutrition?\"\n",
    "print(f'Question: {test_question}')\n",
    "print(f'Response: {evaluate_model(test_question)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba5fdc",
   "metadata": {},
   "source": [
    "## Save Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./malnutrition_assistant_lora')\n",
    "tokenizer.save_pretrained('./malnutrition_assistant_lora')\n",
    "print('Model and tokenizer saved')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
